{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) provides large language models additional helpful information in a prompt that is retrieved when a user submits a query.\n",
    "\n",
    "This guide uses Atlas as a data layer for retrieval, followed by LLM inference using the information queried from our Atlas Dataset via vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the nomic client installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nomic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then login with your Nomic API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nomic login nk-..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Atlas Dataset\n",
    "\n",
    "Let's start with a collection with PDFs and chunk them into snippets to be fetched for retrieval.\n",
    "\n",
    "For this example, we will download and parse PDFs with `docling` from the open-access paper repository arXiv.\n",
    "\n",
    "Make sure `docling` is installed to your python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pdf_pipeline_options = PdfPipelineOptions(do_ocr=False, do_table_structure=False)\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(\n",
    "        pipeline_options=pdf_pipeline_options\n",
    "    )}\n",
    ")\n",
    "chunker = HybridChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing  Attention Is All You Need\n",
      "Downloading and parsing  Deep Residual Learning\n",
      "Downloading and parsing  BERT\n",
      "Downloading and parsing  GPT-3\n",
      "Downloading and parsing  Adam Optimizer\n",
      "Downloading and parsing  GANs\n",
      "Downloading and parsing  U-Net\n",
      "Downloading and parsing  DALL-E 2\n",
      "Downloading and parsing  Stable Diffusion\n"
     ]
    }
   ],
   "source": [
    "# You can replace this with any list of PDFs you want\n",
    "# The file can be a URL or a local filename for a PDF\n",
    "PDFs = [\n",
    "    {'title': \"Attention Is All You Need\", 'file': \"https://arxiv.org/pdf/1706.03762\"},\n",
    "    {'title': \"Deep Residual Learning\", 'file': \"https://arxiv.org/pdf/1512.03385\"},\n",
    "    {'title': \"BERT\", 'file': \"https://arxiv.org/pdf/1810.04805\"},\n",
    "    {'title': \"GPT-3\", 'file': \"https://arxiv.org/pdf/2005.14165\"},\n",
    "    {'title': \"Adam Optimizer\", 'file': \"https://arxiv.org/pdf/1412.6980\"},\n",
    "    {'title': \"GANs\", 'file': \"https://arxiv.org/pdf/1406.2661\"},\n",
    "    {'title': \"U-Net\", 'file': \"https://arxiv.org/pdf/1505.04597\"},\n",
    "    {'title': \"DALL-E 2\", 'file': \"https://arxiv.org/pdf/2204.06125\"},\n",
    "    {'title': \"Stable Diffusion\", 'file': \"https://arxiv.org/pdf/2112.10752\"}\n",
    "]\n",
    "\n",
    "data = []\n",
    "for pdf in PDFs:\n",
    "    print(\"Downloading and parsing\", pdf['title'])\n",
    "    doc = doc_converter.convert(pdf['file']).document\n",
    "    for chunk in chunker.chunk(dl_doc=doc):\n",
    "        chunk_dict = chunk.model_dump()\n",
    "        filename = chunk_dict['meta']['origin']['filename']\n",
    "        heading = chunk_dict['meta']['headings'][0] if chunk_dict['meta']['headings'] else None\n",
    "        page_num = chunk_dict['meta']['doc_items'][0]['prov'][0]['page_no']\n",
    "        data.append(\n",
    "            {\"text\": chunk.text, \"title\": pdf['title'], \"filename\": filename, \"heading\": heading, \"page_num\": page_num}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data to make sure it looks alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'We also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3. Discussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tieleman & Hinton, 2012) with momentum. We vary the β 1 and β 2 when training a variational autoencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden layer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian latent variable. We iterated over a broad range of hyper-parameter choices, i.e. β 1 ∈ [0 , 0 . 9] and β 2 ∈ [0 . 99 , 0 . 999 , 0 . 9999] , and log 10 ( α ) ∈ [ -5 , ..., -1] . Values of β 2 close to 1, required for robustness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction term is important in such cases of slow decay, preventing an adverse effect on optimization.\\nIn Figure 4, values β 2 close to 1 indeed lead to instabilities in training when no bias correction term was present, especially at first few epochs of the training. The best results were achieved with small values of (1 -β 2 ) and bias correction; this was more apparent towards the end of optimization when gradients tends to become sparser as hidden units specialize to specific patterns. In summary, Adam performed equal or better than RMSProp, regardless of hyper-parameter setting.',\n",
       " 'title': 'Adam Optimizer',\n",
       " 'filename': '1412.6980v9.pdf',\n",
       " 'heading': '6.4 EXPERIMENT: BIAS-CORRECTION TERM',\n",
       " 'page_num': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to Atlas\n",
    "\n",
    "Now, we can upload this data to Atlas to create a data map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-29 16:37:37.569\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m133\u001b[0m - \u001b[33m\u001b[1mAn ID field was not specified in your data so one was generated for you in insertion order.\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:39.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m867\u001b[0m - \u001b[1mOrganization name: `nomic`\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:40.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m895\u001b[0m - \u001b[1mCreating dataset `pdf-data-for-rag`\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:40.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mUploading data to Atlas.\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:40.564\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m334\u001b[0m - \u001b[33m\u001b[1mReplacing 9 null values for field heading with string 'null'. This behavior will change in a future version.\u001b[0m\n",
      "1it [00:00,  1.34it/s]\n",
      "\u001b[32m2025-01-29 16:37:41.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1714\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:41.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1m`nomic/pdf-data-for-rag`: Data upload succeeded to dataset`\u001b[0m\n",
      "\u001b[32m2025-01-29 16:37:42.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1301\u001b[0m - \u001b[1mCreated map `pdf-data-for-rag` in dataset `nomic/pdf-data-for-rag`: https://atlas.nomic.ai/data/nomic/pdf-data-for-rag\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nomic import atlas\n",
    "\n",
    "atlas_dataset = atlas.map_data(\n",
    "    data=data,\n",
    "    indexed_field=\"text\",\n",
    "    identifier=\"pdf-data-for-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll get an email when your data map is built!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Over Your Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nomic Atlas vector search API returns the k-most semantically similar items from your Atlas Dataset based on a query. You can read more about how to use this endpoint in our API reference [here](https://docs.nomic.ai/reference/api/query/vector-search).\n",
    "\n",
    "This helper function makes an API call to the Nomic Atlas vector search endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "def retrieve(query: str, dataset_identifier: str, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "    \n",
    "    # load the projection ID for your map\n",
    "    atlas_dataset = AtlasDataset(dataset_identifier)\n",
    "    atlas_map_projection_id = atlas_dataset.maps[0].projection_id\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_map_projection_id,\n",
    "        }\n",
    "    )\n",
    "    return response.json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for this helper function are:\n",
    "\n",
    "• `query`: the text query to search against\n",
    "\n",
    "• `dataset_identifier`: a string of the form \"your_org_name/your_dataset_name\" used to load your dataset from Atlas\n",
    "\n",
    "• `k`: number of similar items to return\n",
    "\n",
    "• `fields`: which fields/columns from your dataset to return in the response\n",
    "\n",
    "Let's inspect the output of `retrieve` on the query \"What metrics are mentioned for evaluation?\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-29 17:09:26.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "dataset_identifier = \"YOUR_ORG_HERE/pdf-data-for-rag\"\n",
    "retrieved_data = retrieve(\n",
    "    query, dataset_identifier, 3, [\"title\", \"heading\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Stable Diffusion',\n",
       "  'heading': 'E.3.5 Efficiency Analysis',\n",
       "  'text': 'For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.',\n",
       "  '_similarity': 0.7273091077804565},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': 'Context → Article:',\n",
       "  'text': \"Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\nFigure G.13: Formatted dataset example for Winograd. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n53\\nFigure G.14: Formatted dataset example for Winogrande. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\nContext\\n→\\nREADING COMPREHENSION ANSWER KEY\",\n",
       "  '_similarity': 0.701439619064331},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': '3 Results',\n",
       "  'text': \"Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH + 20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3's training data. a [RWC + 19]\",\n",
       "  '_similarity': 0.6994962692260742}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End RAG with the Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a retrieval function for our data map, we can now perform end-to-end RAG with Atlas as our intermediate data layer.\n",
    "\n",
    "We will use GPT4o-mini from OpenAI as our LLM in this example. Make sure you have the openai package and an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code Example\n",
    "\n",
    "Here is a complete working example to go from a user query to an LLM response, retrieving data from Atlas as an intermediate step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving data from Atlas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-29 17:12:21.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating response from OpenAI...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "client = OpenAI(\n",
    "#     api_key=\"sk-proj-...\" # add your OpenAI API key here, or set it as an environment variable\n",
    ")\n",
    "\n",
    "def retrieve(query: str, dataset_identifier: str, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "    \n",
    "    # load the projection ID for your map\n",
    "    atlas_dataset = AtlasDataset(dataset_identifier)\n",
    "    atlas_map_projection_id = atlas_dataset.maps[0].projection_id\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_map_projection_id,\n",
    "        }\n",
    "    )\n",
    "    return response.json()['data']\n",
    "\n",
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "print(\"retrieving data from Atlas...\")\n",
    "\n",
    "dataset_identifier = \"YOUR_ORG_HERE/pdf-data-for-rag\"\n",
    "retrieved_data = retrieve(\n",
    "    query, dataset_identifier, 3, [\"title\", \"heading\", \"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"generating response from OpenAI...\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant. Be specific and cite the context you are given\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_data}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the RAG response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What metrics are mentioned for evaluation?\n",
      "\n",
      "A: The context provided mentions several metrics related to sample quality and performance evaluation:\n",
      "\n",
      "1. **Sample Quality Metrics** - Mentioned in the first entry regarding Stable Diffusion, which are computed based on 5,000 samples. These metrics are displayed in figures referenced as Fig. 6, 7, and 17.\n",
      "\n",
      "2. **Cross-Entropy Validation Loss** - In the third entry about GPT-3, performance is measured in terms of cross-entropy validation loss, which follows a power-law trend with the amount of compute used for training.\n",
      "\n",
      "3. **Normalization by Unconditional Probability** - The second entry describes a method used for predicting that involves normalizing by the unconditional probability of each answer, indicating a probabilistic evaluation method.\n",
      "\n",
      "These metrics help assess the quality and performance of the respective models discussed in the context.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q: {query}\\n\\nA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
