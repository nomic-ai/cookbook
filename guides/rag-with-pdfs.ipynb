{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) provides large language models (LLMs) additional information retrieved from databases to help answer a user's query.\n",
    "\n",
    "You can use Atlas as the data layer for retrieval in your RAG application, using the Nomic API to query from an Atlas Dataset via vector search.\n",
    "\n",
    "This guide shows how to parse and chunk text from a collection of PDFs for a RAG application using Nomic Atlas and the OpenAI Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code in this guide, make sure you have `docling`, `nomic`, `openai`, and `requests` installed to your python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling nomic openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, login to `nomic` with your Nomic API key. If you don't have a Nomic API key you can create one [here](https://atlas.nomic.ai/cli-login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nomic login nk-..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Atlas Dataset\n",
    "\n",
    "### Parse PDFs\n",
    "\n",
    "Let's start with a collection with PDFs and chunk them into snippets to be fetched for retrieval. For this example, we will download and parse PDFs with `docling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pdf_pipeline_options = PdfPipelineOptions(do_ocr=False, do_table_structure=False)\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(\n",
    "        pipeline_options=pdf_pipeline_options\n",
    "    )}\n",
    ")\n",
    "chunker = HybridChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing Attention Is All You Need\n",
      "Downloading and parsing Deep Residual Learning\n",
      "Downloading and parsing BERT\n",
      "Downloading and parsing GPT-3\n",
      "Downloading and parsing Adam Optimizer\n",
      "Downloading and parsing GANs\n",
      "Downloading and parsing U-Net\n",
      "Downloading and parsing DALL-E 2\n",
      "Downloading and parsing Stable Diffusion\n"
     ]
    }
   ],
   "source": [
    "# You can replace this with any list of PDFs you want\n",
    "# The file can be a URL or a local filename for a PDF\n",
    "PDFs = [\n",
    "    {'title': \"Attention Is All You Need\", 'file': \"https://arxiv.org/pdf/1706.03762\"},\n",
    "    {'title': \"Deep Residual Learning\", 'file': \"https://arxiv.org/pdf/1512.03385\"},\n",
    "    {'title': \"BERT\", 'file': \"https://arxiv.org/pdf/1810.04805\"},\n",
    "    {'title': \"GPT-3\", 'file': \"https://arxiv.org/pdf/2005.14165\"},\n",
    "    {'title': \"Adam Optimizer\", 'file': \"https://arxiv.org/pdf/1412.6980\"},\n",
    "    {'title': \"GANs\", 'file': \"https://arxiv.org/pdf/1406.2661\"},\n",
    "    {'title': \"U-Net\", 'file': \"https://arxiv.org/pdf/1505.04597\"},\n",
    "    {'title': \"DALL-E 2\", 'file': \"https://arxiv.org/pdf/2204.06125\"},\n",
    "    {'title': \"Stable Diffusion\", 'file': \"https://arxiv.org/pdf/2112.10752\"}\n",
    "]\n",
    "\n",
    "data = []\n",
    "chunk_id = 0\n",
    "for pdf in PDFs:\n",
    "    print(\"Downloading and parsing\", pdf['title'])\n",
    "    doc = doc_converter.convert(pdf['file']).document\n",
    "    for chunk in chunker.chunk(dl_doc=doc):\n",
    "        chunk_dict = chunk.model_dump()\n",
    "        filename = chunk_dict['meta']['origin']['filename']\n",
    "        heading = chunk_dict['meta']['headings'][0] if chunk_dict['meta']['headings'] else None\n",
    "        page_num = chunk_dict['meta']['doc_items'][0]['prov'][0]['page_no']\n",
    "        data.append(\n",
    "            {\"id\": chunk_id, \"text\": chunk.text, \"title\": pdf['title'], \"filename\": filename, \"heading\": heading, \"page_num\": page_num}\n",
    "        )\n",
    "        chunk_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `docling` to parse PDFs provides page number and heading metadata for each document snippet. Additionally, the choice of where one snippet ends and another begins is handled in a smart way by `docling`: it is good at chunking PDFs according to the actual start and end of sections from the PDFs themselves, as opposed to naive chunking which may start/end chunks awkwardly in the middle of a sentence.\n",
    "\n",
    "Here we print an item from `data` to inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 250,\n",
       " 'text': 'We also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3. Discussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tieleman & Hinton, 2012) with momentum. We vary the β 1 and β 2 when training a variational autoencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden layer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian latent variable. We iterated over a broad range of hyper-parameter choices, i.e. β 1 ∈ [0 , 0 . 9] and β 2 ∈ [0 . 99 , 0 . 999 , 0 . 9999] , and log 10 ( α ) ∈ [ -5 , ..., -1] . Values of β 2 close to 1, required for robustness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction term is important in such cases of slow decay, preventing an adverse effect on optimization.\\nIn Figure 4, values β 2 close to 1 indeed lead to instabilities in training when no bias correction term was present, especially at first few epochs of the training. The best results were achieved with small values of (1 -β 2 ) and bias correction; this was more apparent towards the end of optimization when gradients tends to become sparser as hidden units specialize to specific patterns. In summary, Adam performed equal or better than RMSProp, regardless of hyper-parameter setting.',\n",
       " 'title': 'Adam Optimizer',\n",
       " 'filename': '1412.6980v9.pdf',\n",
       " 'heading': '6.4 EXPERIMENT: BIAS-CORRECTION TERM',\n",
       " 'page_num': 8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Atlas Dataset\n",
    "\n",
    "We initialize an `AtlasDataset`, which we can use to store data and retrieve from it for RAG. \n",
    "\n",
    "To create a dataset, we specify a dataset identifier (e.g. \"pdf-data-for-rag\"). Use \"pdf-data-for-rag\" to create it in the organization connected to your API key, or use \"org_name/pdf-data-for-rag\" to create it in a different organization you are a member of.\n",
    "\n",
    "Initialize this `AtlasDataset` with the `id` field from `data` as the `unique_id_field`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 16:15:16.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m867\u001b[0m - \u001b[1mOrganization name: `nomic`\u001b[0m\n",
      "\u001b[32m2025-01-30 16:15:17.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m895\u001b[0m - \u001b[1mCreating dataset `pdf-data-for-rag`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nomic import AtlasDataset\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to create the dataset in the organization connected to your Nomic API key\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to create the dataset in other organizations you are a member of\n",
    "\n",
    "atlas_dataset = AtlasDataset(dataset_identifier, unique_id_field=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Data\n",
    "\n",
    "Add the list called `data` to the `AtlasDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 16:15:17.641\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m334\u001b[0m - \u001b[33m\u001b[1mReplacing 9 null values for field heading with string 'null'. This behavior will change in a future version.\u001b[0m\n",
      "\u001b[32m2025-01-30 16:15:17.642\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mid_field is not a string. Converting to string from int32\u001b[0m\n",
      "1it [00:00,  1.46it/s]\n",
      "\u001b[32m2025-01-30 16:15:18.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1714\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "atlas_dataset.add_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Data Map\n",
    "\n",
    "Create a data map for this dataset. \n",
    "\n",
    "We provide `indexed_field`=\"text\", which Atlas uses to compute embeddings from the values in the `text` column. These embeddings will be that we vector search over for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 16:15:38.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1301\u001b[0m - \u001b[1mCreated map `1` in dataset `nomic/pdf-data-for-rag`: https://atlas.nomic.ai/data/nomic/pdf-data-for-rag\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Atlas Projection 1. Status Embedding. <a target=\"_blank\" href=\"https://atlas.nomic.ai/data/nomic/pdf-data-for-rag/map\">view online</a>"
      ],
      "text/plain": [
       "1: https://atlas.nomic.ai/data/nomic/pdf-data-for-rag"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_dataset.create_index(\n",
    "    indexed_field=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Over Your Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nomic Atlas vector search API returns the k-most semantically similar items from your Atlas Dataset based on a query. You can read more about how to use this endpoint in our API reference [here](https://docs.nomic.ai/reference/api/query/vector-search).\n",
    "\n",
    "This helper function makes an API call to the Nomic Atlas vector search endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "def retrieve(query: str, atlas_dataset: AtlasDataset, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_dataset.maps[0].projection_id,\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid API request or incomplete map - if your map hasn't finished building yet, try again once it's ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for this helper function are:\n",
    "\n",
    "• `query`: the text query to search against\n",
    "\n",
    "• `atlas_dataset`: the `AtlasDataset` to retrieve from\n",
    "\n",
    "• `k`: number of similar items to return\n",
    "\n",
    "• `fields`: which fields/columns from your dataset to return in the response\n",
    "\n",
    "Let's inspect the output of `retrieve` on the query \"What metrics are mentioned for evaluation?\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "retrieved_data = retrieve(\n",
    "    query, atlas_dataset, 3, [\"title\", \"heading\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Stable Diffusion',\n",
       "  'heading': 'E.3.5 Efficiency Analysis',\n",
       "  'text': 'For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.',\n",
       "  '_similarity': 0.7273091077804565},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': 'Context → Article:',\n",
       "  'text': \"Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\nFigure G.13: Formatted dataset example for Winograd. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n53\\nFigure G.14: Formatted dataset example for Winogrande. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\nContext\\n→\\nREADING COMPREHENSION ANSWER KEY\",\n",
       "  '_similarity': 0.701439619064331},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': '3 Results',\n",
       "  'text': \"Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH + 20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3's training data. a [RWC + 19]\",\n",
       "  '_similarity': 0.6994962692260742}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with the Atlas Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a retrieval function for our data map, we can now perform RAG with Atlas as our intermediate data layer we retrieve relevant data from.\n",
    "\n",
    "We will use GPT4o-mini from OpenAI as our LLM in this example. Make sure you have an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 16:17:47.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "client = OpenAI(\n",
    "    # api_key=\"sk-proj-...\" # add your OpenAI API key here, or set it as an environment variable\n",
    ")\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to retrieve from datasets in in the organization connected to your Nomic API key\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to retrieve from datasets in other organizations\n",
    "atlas_dataset = AtlasDataset(dataset_identifier, unique_id_field=\"id\")\n",
    "\n",
    "def retrieve(query: str, atlas_dataset: AtlasDataset, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_dataset.maps[0].projection_id,\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid API request or incomplete map - if your map hasn't finished building yet, try again once it's ready.\")\n",
    "\n",
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "retrieved_data = retrieve(\n",
    "    query, atlas_dataset, 3, [\"title\", \"heading\", \"text\"]\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant. Be specific and cite the context you are given\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_data}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the RAG response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What metrics are mentioned for evaluation?\n",
      "\n",
      "A: In the context of the \"Stable Diffusion\" section, the metrics mentioned for evaluation are \"sample quality metrics,\" which are computed based on 5,000 samples. It indicates that these metrics are relevant for assessing the efficiency of the models. In the \"GPT-3\" context, evaluation metrics are implied through the descriptions of figures, such as comparing the \"probability of the completion given a correct and incorrect context\" in the Winograd and Winogrande datasets.\n",
      "\n",
      "To summarize, the metrics specifically mentioned for evaluation include:\n",
      "- Sample quality metrics (in Stable Diffusion)\n",
      "- Probability comparisons in the evaluation methods for the Winograd and Winogrande datasets (in GPT-3).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q: {query}\\n\\nA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
