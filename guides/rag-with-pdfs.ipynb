{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) provides large language models (LLMs) additional information retrieved from databases to help answer a user's query.\n",
    "\n",
    "You can use Atlas as the data layer for retrieval in your RAG application, using the Nomic API to query from an Atlas Dataset via vector search.\n",
    "\n",
    "This guide shows how to parse and chunk text from a collection of PDFs for a RAG application using Nomic Atlas and the OpenAI Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code in this guide, make sure you have `docling`, `nomic`, `openai`, and `requests` installed to your python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling nomic openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, login to `nomic` with your Nomic API key. If you don't have a Nomic API key you can create one [here](https://atlas.nomic.ai/cli-login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nomic login nk-..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Atlas Dataset\n",
    "\n",
    "Let's start with a collection with PDFs and chunk them into snippets to be fetched for retrieval.\n",
    "\n",
    "For this example, we will download and parse PDFs with `docling` from the open-access paper repository arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pdf_pipeline_options = PdfPipelineOptions(do_ocr=False, do_table_structure=False)\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(\n",
    "        pipeline_options=pdf_pipeline_options\n",
    "    )}\n",
    ")\n",
    "chunker = HybridChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing Attention Is All You Need\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing Deep Residual Learning\n",
      "Downloading and parsing BERT\n",
      "Downloading and parsing GPT-3\n",
      "Downloading and parsing Adam Optimizer\n",
      "Downloading and parsing GANs\n",
      "Downloading and parsing U-Net\n",
      "Downloading and parsing DALL-E 2\n",
      "Downloading and parsing Stable Diffusion\n"
     ]
    }
   ],
   "source": [
    "# You can replace this with any list of PDFs you want\n",
    "# The file can be a URL or a local filename for a PDF\n",
    "PDFs = [\n",
    "    {'title': \"Attention Is All You Need\", 'file': \"https://arxiv.org/pdf/1706.03762\"},\n",
    "    {'title': \"Deep Residual Learning\", 'file': \"https://arxiv.org/pdf/1512.03385\"},\n",
    "    {'title': \"BERT\", 'file': \"https://arxiv.org/pdf/1810.04805\"},\n",
    "    {'title': \"GPT-3\", 'file': \"https://arxiv.org/pdf/2005.14165\"},\n",
    "    {'title': \"Adam Optimizer\", 'file': \"https://arxiv.org/pdf/1412.6980\"},\n",
    "    {'title': \"GANs\", 'file': \"https://arxiv.org/pdf/1406.2661\"},\n",
    "    {'title': \"U-Net\", 'file': \"https://arxiv.org/pdf/1505.04597\"},\n",
    "    {'title': \"DALL-E 2\", 'file': \"https://arxiv.org/pdf/2204.06125\"},\n",
    "    {'title': \"Stable Diffusion\", 'file': \"https://arxiv.org/pdf/2112.10752\"}\n",
    "]\n",
    "\n",
    "data = []\n",
    "chunk_id = 0\n",
    "for pdf in PDFs:\n",
    "    print(\"Downloading and parsing\", pdf['title'])\n",
    "    doc = doc_converter.convert(pdf['file']).document\n",
    "    for chunk in chunker.chunk(dl_doc=doc):\n",
    "        chunk_dict = chunk.model_dump()\n",
    "        filename = chunk_dict['meta']['origin']['filename']\n",
    "        heading = chunk_dict['meta']['headings'][0] if chunk_dict['meta']['headings'] else None\n",
    "        page_num = chunk_dict['meta']['doc_items'][0]['prov'][0]['page_no']\n",
    "        data.append(\n",
    "            {\"id\": chunk_id, \"text\": chunk.text, \"title\": pdf['title'], \"filename\": filename, \"heading\": heading, \"page_num\": page_num}\n",
    "        )\n",
    "        chunk_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `docling` to parse PDFs provides page number and heading metadata for each document snippet. Additionally, the choice of where one snippet ends and another begins is handled in a smart way by `docling`: it is good at chunking PDFs according to the actual start and end of sections from the PDFs themselves, as opposed to naive chunking which may start/end chunks awkwardly in the middle of a sentence.\n",
    "\n",
    "Here we print an item from `data` to inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 250,\n",
       " 'text': 'We also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3. Discussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tieleman & Hinton, 2012) with momentum. We vary the β 1 and β 2 when training a variational autoencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden layer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian latent variable. We iterated over a broad range of hyper-parameter choices, i.e. β 1 ∈ [0 , 0 . 9] and β 2 ∈ [0 . 99 , 0 . 999 , 0 . 9999] , and log 10 ( α ) ∈ [ -5 , ..., -1] . Values of β 2 close to 1, required for robustness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction term is important in such cases of slow decay, preventing an adverse effect on optimization.\\nIn Figure 4, values β 2 close to 1 indeed lead to instabilities in training when no bias correction term was present, especially at first few epochs of the training. The best results were achieved with small values of (1 -β 2 ) and bias correction; this was more apparent towards the end of optimization when gradients tends to become sparser as hidden units specialize to specific patterns. In summary, Adam performed equal or better than RMSProp, regardless of hyper-parameter setting.',\n",
       " 'title': 'Adam Optimizer',\n",
       " 'filename': '1412.6980v9.pdf',\n",
       " 'heading': '6.4 EXPERIMENT: BIAS-CORRECTION TERM',\n",
       " 'page_num': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize `AtlasDataset`\n",
    "\n",
    "We initialize an `AtlasDataset`, which we can use to define a dataset, add data, retrieve from it during RAG, and if desired add new data later on.\n",
    "\n",
    "#### Dataset Identifier\n",
    "\n",
    "Choose a name for your dataset (e.g. \"pdf-data-for-rag\").\n",
    "\n",
    "Use \"dataset_name\" to create datasets in the organization connected to your API key. To specify a different org you are in, use \"org_name/dataset_name\".\n",
    "\n",
    "#### Unique ID field\n",
    "\n",
    "Initialize this `AtlasDataset` with the `id` field from `data` as the `unique_id_field`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 15:50:05.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m867\u001b[0m - \u001b[1mOrganization name: `nomic`\u001b[0m\n",
      "\u001b[32m2025-01-30 15:50:05.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m895\u001b[0m - \u001b[1mCreating dataset `pdf-data-for-rag`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nomic import AtlasDataset\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to create the dataset in the organization connected to your Nomic API key\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to create the dataset in other organizations you are a member of\n",
    "dataset = AtlasDataset(dataset_identifier, unique_id_field=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `data` to `AtlasDataset`\n",
    "\n",
    "Add the list called `data` to the `AtlasDataset`\n",
    "\n",
    "Create a data map from the dataset with the `text` field. \n",
    "\n",
    "This computes embeddings from the values in the `text` column, which we will vector search over for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 15:50:06.012\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m334\u001b[0m - \u001b[33m\u001b[1mReplacing 9 null values for field heading with string 'null'. This behavior will change in a future version.\u001b[0m\n",
      "\u001b[32m2025-01-30 15:50:06.013\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mid_field is not a string. Converting to string from int32\u001b[0m\n",
      "1it [00:00,  1.39it/s]\n",
      "\u001b[32m2025-01-30 15:50:06.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1714\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n",
      "\u001b[32m2025-01-30 15:50:08.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1301\u001b[0m - \u001b[1mCreated map `1` in dataset `nomic/pdf-data-for-rag`: https://atlas.nomic.ai/data/nomic/pdf-data-for-rag\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Atlas Projection 1. Status Embedding. <a target=\"_blank\" href=\"https://atlas.nomic.ai/data/nomic/pdf-data-for-rag/map\">view online</a>"
      ],
      "text/plain": [
       "1: https://atlas.nomic.ai/data/nomic/pdf-data-for-rag"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add data to AtlasDataset\n",
    "dataset.add_data(data)\n",
    "\n",
    "# create a data map from the AtlasDataset\n",
    "# using the 'text' field to generate embeddings\n",
    "dataset.create_index(\n",
    "    indexed_field=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 15:43:39.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to retrieve from datasets in the organization connected to your Nomic API key\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to retrieve from datasets in other organizations\n",
    "atlas_dataset = AtlasDataset(dataset_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Over Your Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nomic Atlas vector search API returns the k-most semantically similar items from your Atlas Dataset based on a query. You can read more about how to use this endpoint in our API reference [here](https://docs.nomic.ai/reference/api/query/vector-search).\n",
    "\n",
    "This helper function makes an API call to the Nomic Atlas vector search endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "def retrieve(query: str, atlas_dataset: AtlasDataset, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_dataset.maps[0].projection_id,\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid API request or incomplete map - if your map hasn't finished building yet, try again once it's ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for this helper function are:\n",
    "\n",
    "• `query`: the text query to search against\n",
    "\n",
    "• `atlas_dataset`: the `AtlasDataset` to retrieve from\n",
    "\n",
    "• `k`: number of similar items to return\n",
    "\n",
    "• `fields`: which fields/columns from your dataset to return in the response\n",
    "\n",
    "Let's inspect the output of `retrieve` on the query \"What metrics are mentioned for evaluation?\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 15:58:08.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to retrieve from datasets in the organization connected to your Nomic API key\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to retrieve from datasets in other organizations\n",
    "atlas_dataset = AtlasDataset(dataset_identifier)\n",
    "\n",
    "retrieved_data = retrieve(\n",
    "    query, atlas_dataset, 3, [\"title\", \"heading\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Stable Diffusion',\n",
       "  'heading': 'E.3.5 Efficiency Analysis',\n",
       "  'text': 'For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.',\n",
       "  '_similarity': 0.7273091077804565},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': 'Context → Article:',\n",
       "  'text': \"Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\nFigure G.13: Formatted dataset example for Winograd. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n53\\nFigure G.14: Formatted dataset example for Winogrande. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\nContext\\n→\\nREADING COMPREHENSION ANSWER KEY\",\n",
       "  '_similarity': 0.701439619064331},\n",
       " {'title': 'GPT-3',\n",
       "  'heading': '3 Results',\n",
       "  'text': \"Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH + 20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3's training data. a [RWC + 19]\",\n",
       "  '_similarity': 0.6994962692260742}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with the Atlas Data Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a retrieval function for our data map, we can now perform RAG with Atlas as our intermediate data layer we retrieve relevant data from.\n",
    "\n",
    "We will use GPT4o-mini from OpenAI as our LLM in this example. Make sure you have an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving data from Atlas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 15:58:26.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m804\u001b[0m - \u001b[1mLoading existing dataset `nomic/pdf-data-for-rag`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating response from OpenAI...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from nomic import AtlasDataset\n",
    "\n",
    "client = OpenAI(\n",
    "    # api_key=\"sk-proj-...\" # add your OpenAI API key here, or set it as an environment variable\n",
    ")\n",
    "\n",
    "def retrieve(query: str, dataset_identifier: str, k: int, fields: list[str]) -> list:\n",
    "    \"\"\"Retrieve semantically similar items from an Atlas Dataset based on a query.\"\"\"\n",
    "    \n",
    "    # load the projection ID for your map\n",
    "    atlas_dataset = AtlasDataset(dataset_identifier)\n",
    "    atlas_map_projection_id = atlas_dataset.maps[0].projection_id\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://api-atlas.nomic.ai/v1/query/topk',\n",
    "        headers={'Authorization': f'Bearer {os.environ.get(\"NOMIC_API_KEY\")}'},\n",
    "        json={\n",
    "            'query': query,\n",
    "            'k': k,\n",
    "            'fields': fields,\n",
    "            'projection_id': atlas_dataset.maps[0].projection_id,\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid API request or incomplete map - if your map hasn't finished building yet, try again once it's ready.\")\n",
    "\n",
    "query = \"What metrics are mentioned for evaluation?\"\n",
    "\n",
    "print(\"retrieving data from Atlas...\")\n",
    "\n",
    "dataset_identifier = \"pdf-data-for-rag\" # to load datasets in your organization\n",
    "# dataset_identifier = \"<ORG_NAME>/pdf-data-for-rag\" # to load datasets in other organizations\n",
    "\n",
    "retrieved_data = retrieve(\n",
    "    query, dataset_identifier, 3, [\"title\", \"heading\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"generating response from OpenAI...\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant. Be specific and cite the context you are given\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_data}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the RAG response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What metrics are mentioned for evaluation?\n",
      "\n",
      "A: The context mentions sample quality metrics related to the evaluation of models in **Stable Diffusion**. Specifically, it refers to metrics that are plotted in figures and computed based on **5,000 samples**. The mention of varying results compared to those in tables indicates that these metrics are critical for assessing model performance. \n",
      "\n",
      "In the **GPT-3** reference, it discusses the evaluation methods that involve comparing the probability of the completion given correct and incorrect contexts, specifically within the context of the Winograd and Winogrande datasets. However, it does not explicitly list specific metrics beyond indicating they normalize probabilities and employ a 'partial' evaluation method.\n",
      "\n",
      "Thus, the specific metrics mentioned include degree of performance measured in terms of **cross-entropy validation loss** in the context of GPT-3, and general sample quality metrics in Stable Diffusion.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q: {query}\\n\\nA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
